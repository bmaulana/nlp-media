\documentclass{report}
\usepackage{setspace}
%\usepackage{subfigure}

\pagestyle{plain}
\usepackage{amssymb,graphicx,color}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{a4wide}
\usepackage{amsmath}
\usepackage[super]{nth}

\newtheorem{theorem}{THEOREM}
\newtheorem{lemma}[theorem]{LEMMA}
\newtheorem{corollary}[theorem]{COROLLARY}
\newtheorem{proposition}[theorem]{PROPOSITION}
\newtheorem{remark}[theorem]{REMARK}
\newtheorem{definition}[theorem]{DEFINITION}
\newtheorem{fact}[theorem]{FACT}

\newtheorem{problem}[theorem]{PROBLEM}
\newtheorem{exercise}[theorem]{EXERCISE}
\def \set#1{\{#1\} }

\newenvironment{proof}{
PROOF:
\begin{quotation}}{
$\Box$ \end{quotation}}



\newcommand{\nats}{\mbox{\( \mathbb N \)}}
\newcommand{\rat}{\mbox{\(\mathbb Q\)}}
\newcommand{\rats}{\mbox{\(\mathbb Q\)}}
\newcommand{\reals}{\mbox{\(\mathbb R\)}}
\newcommand{\ints}{\mbox{\(\mathbb Z\)}}

%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{{\vspace{-14em} \includegraphics[scale=0.4]{ucl_logo.png}}\\
{{\Huge Using natural language processing to develop a pipeline to analyse media representation of people with disabilities in Web-based news articles}}\\
{\large Collection and filtering of Web-based news articles, comparison of open-source sentiment models, and applications of the technology
}\\
}
\date{Submission date: \nth{30} April 2018}
\author{Bagus Maulana\thanks{
{\bf Disclaimer:}
This report is submitted as part requirement for the MEng Degree in Computer Science at UCL. It is
substantially the result of my own work except where explicitly indicated in the text.
\emph{The report may be freely copied and distributed provided the source is explicitly acknowledged}}
\\ \\
MEng Computer Science\\ \\
Catherine Holloway, Nicholas Firth}



\begin{document}
 
\onehalfspacing
\maketitle
\begin{abstract}

\textbf{Report Title:}  Using natural language processing to develop a pipeline to analyse media representation of people with disabilities in Web-based news articles: Collection and filtering of Web-based news articles, comparison of open-source sentiment models, and applications of the technology

\textbf{Author’s Name:} Bagus Maulana

\textbf{Supervisor’s Name:} Catherine Holloway, Nicholas Firth

\textbf{Date and Year of Submission:} \nth{30} April 2018\\

%% Three paragraphs:

%% What the project is about, the principle aims and goals, and specific challenges.

%% How you carried out the project and what work it involved. How you went about meeting the project goals.

%% The results and achievements of the project. What the outcome is.

%% Half a page’s length.}
\end{abstract}
\tableofcontents
\setcounter{page}{1}


\chapter{Introduction}

%% Outline the problem you are working on, why it is interesting and what the challenges are.

% Talk about NLP
Natural language processing (NLP) encompasses a wide range of computational techniques for machine understanding of human (natural) language that are often used alongside each other.
The review article \cite{Jumping NLP Curves} defines natural language processing as 'a theory-motivated range of computational techniques for the automatic analysis and representation of human language.'
The techniques that fall under the umbrella natural language processing include word tokenisation, probabilistic language modelling, translation, part-of-speech parsing, sentiment analysis (or opinion mining), text classification/categorisation, and topic modelling, among other things.
The computational models used in natural language processing range from simple rule-based models (e.g. splitting a sentence on whitespace to tokenise words) to statistical machine learning and deep learning models.
Natural language processing is now applied for various everyday technologies, for example, information retrieval for search engines such as Google and Bing, and categorisation and topic modelling for recommendation engines used to suggest 'similar' articles.

The obvious advantage of natural language processing is that machines can process vast bodies of human-created literature (books, articles, posts, e-mails, messages, etc.) much faster than humans can, processing thousands or millions of documents per second. 
This allows for high-level quantitative analyses of all documents in a vast corpora for a given domain to be feasible, which can uncover information previously inaccessible by only reading and generalising from a small sample of documents.
For example, this level of quantitative analysis can uncover trends and patterns within a given domain (e.g. how does the popularity of the term 'mentally ill' increase or decrease year-on-year in English news media?). 

Natural language processing covers three main 'curves' or areas:  syntax, semantics, and pragmatics (narratives, understanding). 
Syntax specifies the way symbols (words, terms, tokens, or n-grams) and groups of symbols are arranged and whether they are well-formed in an expression, whereas semantics specifies what these expressions mean, and pragmatics specifies contextual information \cite{Jumping NLP Curves}.
Contemporary (or 'traditional') approaches to natural language processing mainly focus on syntactic analysis, due to the relative ease of extracting syntactic features of text such as term frequency, word co-occurrence, and part-of-speech tags, compared to extracting logical expressions and networks necessary for semantic analysis.
However, syntactic analysis is much more limited as it often misses information such as the (semantic) context of a word (e.g. "one" in "there's no one there" (referring to a person) vs "we have only one car" (referring to a quantity)).
This paper will focus on mainly syntactic techniques and features, as these are more relevant to this domain of high-level topic matching and sentiment analysis that is feasible with current technology at this scale.
% Just give a short summary, expand this later in Chapter 2

% Other work in using NLP for media analysis
Various other studies have attempted to utilise natural language processing to perform high-level analyses in the domain of news media.
The research done in \cite{Content analysis of 150 years of British periodicals} assembled a vast corpus of regional newspapers in the United Kingdom spanning 150 years to detect long-term patterns of cultural change (e.g. increase of female representation in the news, or when trains overtook horses for transportation) by analysing n-gram trends and named entities. 
More specifically, in the domain of how groups of people are represented in the media, studies such as \cite{Are You a Racist or Am I Seeing Things?} has attempted to use features based on natural language processing (such as n-grams and part-of-speech tags) to classify racist and sexist posts in social media, although there is still a research gap in this area (especially for news articles, and/or specific groups such as people with disabilities or mental illnesses).
% Just give a short summary, expand this later in Chapter 2

% Why is it interesting & what the challenges are
Applying natural language processing to perform meta-analyses over large text corpora has various interesting potential applications in improving our understanding of the human world - for example, to detect macroscopic cultural shifts as in \cite{Content analysis of 150 years of British periodicals}.
In particular, the representation of specific groups, such as people with disabilities, has been a popular research theme for psychologists, sociologists, and others.
For example, the paper \cite{Depictions of Mental Illness in Print Media} analysed a sample of 600 print articles relating to mental illnesses in New Zealand and categorised them to positive and negative depictions, and the predominant themes thereof (e.g. criminality (negative), educational accomplishments (positive)).
Applying natural language processing to this area of research would allow the possibility of discovering higher-level trends, by computationally analysing a much larger sample of articles and identify trends by varying dependent variables such as year of publication and publisher.
In this research, a sample of 305,113 news articles (51,177 after filtering off-topic articles) from British online news sources are used.
However, challenges remain as syntax-based statistical natural language processing approaches tend to be more limited in scope and is prone to false positives and negatives, and mitigating these factors is currently an open area of research.
% Just give a short summary, expand this later and give more papers in Chapter 2
% google 'media representation of mental illness', 'media representation of disability'

%% List your aims and goals. An aim is something you intend to achieve (e.g., learn a new programming language and apply it in solving the problem), while a goal is something specific you expect to deliver (e.g., a working application with a particular set of features).
The aim of this project is to utilise these natural language processing computational techniques in order to perform a high-level meta-analysis of literature available in the public news media available online.
More specifically, to gather online news articles relating to people with disabilities in British media, and perform natural language processing analyses at scale to identify trends such as term popularity (e.g. 'suffer from ...' vs 'with ...') and variation in positive/negative sentiment.

The goal of this project is to develop a computational pipeline capable of performing this analysis of online media end-to-end.
Given a list of topic consisting of keywords and query terms, this pipeline covers the task of web crawling and scraping, using public APIs if possible, to collect a dataset of news articles; filtering off-topic articles for the given keywords; matching relevant sentences referring to a keyword; performing sentiment analysis (using publicly available open-source libraries) on these sentences; and producing relevant plots to show applications of this technology.
This pipeline will be available open source on GitHub (https://github.com/bmaulana/nlp-media)  % should this be a bib reference?

%% Give an overview of how you carried out the project (e.g., an iterative approach).
This project was carried out in a step-by-step approach. The pipeline was developed as four individual components: a web scraper and crawler for data collection given a list of queries, a filter to remove irrelevant articles given a list of key terms, a parser to pattern-match relevant sentences given key terms, a sentiment scorer (and results analysis/comparison of different open-source scorers on this domain), and a script to perform statistical analysis on the results and produce relevant plots. Each component's output is piped to the next component's input by saving its output to a JSON file and having the next component read the previous component's output file, which ensures computation can be 'resumed' without re-running the previous component. A main pipeline script connects these components together by calling them in order for each topic and supported Web source (Daily Mail, Daily Express, Guardian).

%% A brief overview of the rest of the chapters in the report (a guide to the reader of the overall structure of the report).
The body of this report is subdivided into four sections: context, requirements and analysis, design and implementation, and results evaluation.
% TODO finish while finishing rest-of-report

% Brief results & conclusions
% TODO finish after finishing rest-of-report

%% This chapter is relatively short (2-4 pages) and must leave the reader very clear on what the project is about and what your goals are


\chapter{Context}

%% This chapter should cover background information, related work, research done, and tools or software selected for use in the project.

%% Provide necessary context and background information to describe how your project relates to what is already known or available.

%% A description of the research carried out to learn out about the nature of the problem(s) being investigated and potential solutions. The form of the research will vary widely depending on the kind of project. For example, it might involve searching through research publications and online resources, or might involve an exploration of design possibilities for a user interface or program structure.

%% The sources of information you are drawing on (papers, books, websites, etc.) should all be cited or referenced clearly. In addition, state how each source relates to your work and avoid the temptation to pad out the chapter by including sources that you didn’t make use of during the project.

%% If relevant, a survey of similar solutions, programs or applications to yours, and how yours is differentiated.

%% Introduce the software, programming languages, library code, frameworks and other tools that you are using. Discuss choices and make clear which you made use of and why.

%% You should not include well known things (e.g., HTML or Java) or try to give tutorials on how to use a tool or code library (use references to books and websites for that information). Everything you include should be directly relevant to your work and the relationship made clear. This chapter is likely to be fairly substantial, perhaps 8-10 pages.


\chapter{Requirements and Analysis}

%% Give the detailed problem statement. This elaborates on what you may have included in the introduction chapter, and represents the starting point from which requirements were derived.

%% A structured list of requirements.

%% Use cases (a use diagram and list of use case titles, with the full use cases appearing in the appendix).

%% Results of analysing the requirements to extract information. For example, data modelling to find the data to be stored (ER diagram), views/web pages needed and so on.

%% The level of detail of the requirements and use cases will depend on the nature of your project. If you are doing a Software Engineering based design and implementation project, then they will need to be done thoroughly. If there is a substantial body of requirements and use cases, then a summary should be given in the chapter, with the full set included in an appendix section.
%% If your project is not Software Engineering oriented, then you still need to describe the requirements you are working to and relevant analysis information. Use cases may not be needed or be relevant.
%% The analysis part of the chapter is what you did to map the requirements information into the first pass design. You can think of analysis as the first stage of design, and the purpose is to show how the requirements were used to inform the design. The length of this chapter depends on the kind of project, but you are typically looking at 5-6 pages.


\chapter{Design and Implementation}

%% Describe the design of what you have created.

%% Start with the application architecture, giving its overall structure and the components that make up that structure.

%% Give a description of the design of each of the components that make up the architecture.

%% Include the database or storage representation.

%% Provide implementation details as necessary.

%% As with other chapters, the structure and contents of this chapter will depend on the nature of your project, so the list above is only a suggestion, not a fixed requirement.

%% Find an ordering and form of words so that the design is clear, focusing on the interesting design decisions. For example, what were the alternatives, why select one particular solution? You have a limited number of pages so be selective about details. Also remember that someone (your examiners!) has to read this so don’t overwhelm them with intricate descriptions of everything that only you can follow – but do make sure the key details of the solution are in place. Use appropriate terminology and demonstrate that you have a good understanding of the Computer Science principles involved.
%% You can use diagrams and screen shots to help explain the design but don’t overuse them. Diagrams and screen shots should add information, not duplicate what is written in the text, and definitely avoid page after page of diagrams as this will disrupt the flow of your text. Where relevant, UML diagrams can certainly be used but, again, don’t flood the chapter with diagrams. Additional diagrams can always be included in an appendix section. It is not the case that a full set of UML diagrams must be provided for a software development project, and they shouldn’t be added in the belief that there must be UML diagrams to do a good project. Think about what you need to communicate and use UML diagrams if and when they fit the need.
%% It may be useful to include sections of code to highlight how a particular algorithm is implemented or how an interesting programming problem was solved. However, avoid lengthy sections of code, as this can also disrupt the flow of the text. Also make sure that your code fragments are readable, easy to follow and properly laid out. It may be better to use pseudo-code rather than actual code, especially when describing an algorithm. If you need to make use of longer sections of code, you can put the code in the appendix and reference it from the text.
%% An alternative way to organise the content of both this chapter and the preceding one, suitable for some projects, is to have a sequence of chapters or sections for each major iteration of the project. This allows the progression of the project to be shown, with each iteration building on the last, and the opportunity for interesting discussion about the decisions that needed to be made.
%% This is a core chapter in your report and will usually be quite substantial, 10 pages or more.


\chapter{Results Evaluation}

%% Describe your testing strategy (unit, functional, acceptance testing; and how they are carried out). How were test cases selected?

%% Examples of specific tests and how they were carried out (e.g., using mock objects to break dependencies). Focus on the interesting cases.

%% A summary of the test results and what coverage was achieved. Detailed test reports should appear in the appendix, if they add useful information or you want to demonstrate the kinds of tests and coverage achieved.

%% If your project requires substantial evaluation of data and results, evaluation of algorithms, or other forms of testing that are not code-based, then adapt this chapter to suit.
%% This chapter will typically be 2-4 pages in length but could be more depending on the depth of testing done. If you need to do a detailed evaluation for a more mathematical or theory-based project, then this chapter could well be more substantial.


\chapter{Conclusions}
%% Wrap-up and final thoughts on your project. 
%% This chapter is typically 2-4 pages long but could be longer if the project work requires more extensive evaluation.

\section{Achievements}
%% Summarise the achievements to confirm the project goals have been met.
%% A summary of what the project has achieved. Make sure that you address each goal set out in the Introduction chapter, to show that you have achieved what you claimed you would. Don’t leave any loose ends

\section{Evaluation}
%% Evaluation of the work (this may be in a separate chapter if there is substantial evaluation).
%% A critical evaluation of the results of the project (e.g., how well were the goals met, is the application fit for purpose, has good design and implementation practice been followed, was the right implementation technology chosen and so on).

\section{Future Work}
%% How the project might be continued, but don't give the impression you ran out of time!
%% Future work. How could the project be developed if you had another 6 months. Take care to differentiate between what you have done to satisfy your stated project goals, and work that could be done to meet extended goals.


\appendix

\begin{thebibliography}{HHM99}
  
\bibitem[Cam14]{Jumping NLP Curves}
E.~Cambria and B.~White.
\newblock Jumping NLP Curves: A Review of Natural Language Processing Research
\newblock {\em IEEE Computational Intelligence Magazine}, 9(2):  48--57, 2014.

\bibitem[Lan17]{Content analysis of 150 years of British periodicals}
T.~Lansdall-Welfare, et al.
\newblock Content analysis of 150 years of British periodicals
\newblock {\em Proceedings of the National Academy of Sciences}, 114(4):  457--465, 2017.

\bibitem[Zee16]{Are You a Racist or Am I Seeing Things?}
W.~Zeerak.
\newblock Are You a Racist or Am I Seeing Things? Annotator Influence on Hate Speech Detection on Twitter
\newblock {\em Proceedings of 2016 EMNLP Workshop on Natural Language Processing and Computational Social Science}:  138--142, 2016.

\bibitem[Cov02]{Depictions of Mental Illness in Print Media}
J.~Coverdale, N.~Raymond, and C.~Donna.
\newblock Depictions of Mental Illness in Print Media: A Prospective National Sample
\newblock {\em Australian \& New Zealand Journal of Psychiatry}, 36(5):  697--700, 2002.

\end{thebibliography}

\chapter{Other appendices, e.g., code listing} %% TODO
\emph{Put your appendix sections here} %% TODO

\end{document}